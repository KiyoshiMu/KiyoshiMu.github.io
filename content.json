{"meta":{"title":"Portfolio","subtitle":"From WHIM to REALITY","description":"My humble portfolio","author":"Kiyoshi Mu","url":"https://kiyoshimu.github.io","root":"/"},"pages":[{"title":"About","date":"2018-12-13T03:14:36.000Z","updated":"2023-03-18T15:07:51.184Z","comments":false,"path":"about/index.html","permalink":"https://kiyoshimu.github.io/about/index.html","excerpt":"","text":"It's a beautiful day, right? Let's chat bot_ui_ini()"},{"title":"Comment","date":"2018-12-21T04:13:48.000Z","updated":"2023-03-18T15:07:51.184Z","comments":true,"path":"comment/index.html","permalink":"https://kiyoshimu.github.io/comment/index.html","excerpt":"","text":"念两句诗 叙别梦、扬州一觉。 【宋代】吴文英《夜游宫·人去西楼雁杳》"},{"title":"Tags","date":"2018-12-13T03:14:16.000Z","updated":"2023-03-18T15:07:51.184Z","comments":true,"path":"tags/index.html","permalink":"https://kiyoshimu.github.io/tags/index.html","excerpt":"","text":""},{"title":"Rss","date":"2018-12-21T04:09:03.000Z","updated":"2023-03-18T15:07:51.184Z","comments":true,"path":"rss/index.html","permalink":"https://kiyoshimu.github.io/rss/index.html","excerpt":"","text":""}],"posts":[{"title":"Automated Bone Marrow Cytology Using Deep Learning","slug":"automated-bone-marrow-cytology","date":"2022-06-17T04:00:00.000Z","updated":"2023-03-18T17:24:05.928Z","comments":false,"path":"2022/06/17/automated-bone-marrow-cytology/","link":"","permalink":"https://kiyoshimu.github.io/2022/06/17/automated-bone-marrow-cytology/","excerpt":"","text":"Original paper: Automated bone marrow cytology using deep learning to generate a histogram of cell types IntroductionBone marrow cytology is a critical diagnostic tool used in hematology to identify hematological disorders, such as leukemia, myeloma, and lymphoma. However, the manual process is time-consuming, requiring trained experts to examine bone marrow aspirate digital whole slide images, and is prone to inter-observer variability, which can lead to delayed or incorrect diagnoses. An unmet need for innovative technologies to support the diagnosis of hematological disorders prompted researchers to develop an automated bone marrow cytology system. Main Problems How to distinguish ROI and Non-ROI patches in bone marrow aspirate digital whole slide images (WSI)? How to identify specific types of cells on patches? Solutions AppliedWe applied DenseNet-121 to distinguish ROI and Non-ROI, and trained YOLO (You Only Look Once)v4 to identify specific types of cells on patches. The overall process: DenseNet-121 for ROI detectionDenseNet-121 is a convolutional neural network architecture that has shown remarkable performance on various computer vision tasks, including image classification, object detection, and segmentation. It is a popular model that has been pre-trained on a large dataset of natural images, which makes it a strong candidate for fine-tuning as a region of interest (ROI) detection model. ROI detection is the process of identifying regions of an image that are relevant to a specific task, such as object detection, segmentation, or classification. Here, ROI detection is essential for categorizing cells in medical images. Fine-tuning DenseNet-121 for ROI detection involves training the network on a small dataset of labeled images that are specific to the task of interest, i.e., patches with annotations of regions of interest. During training, the network is updated to optimize for the ROI classification task using the labeled data. The advantage of fine-tuning DenseNet-121 for ROI detection is that it can leverage the pre-trained weights of the network, which have been learned on a large dataset of natural images. This pre-training provides a strong initialization for the network, which can improve the performance of the ROI detection model with a limited amount of labeled data. Additionally, the densely connected architecture of DenseNet-121 enables the network to extract features from different layers, which is beneficial for ROI detection tasks where multiple levels of features are required. The process of region of interest (ROI) detection: YOLOv4 for cell detection and classificationYOLO (You Only Look Once) is a popular object detection algorithm that can be trained to detect and classify objects in an image. YOLOv4 is a single-stage object detection algorithm, which means that it predicts bounding boxes and class probabilities in a single forward pass of the neural network. This architecture enables YOLOv4 to be fast and efficient, making it suitable for our real-time applications. To train YOLOv4 to identify specific types of cells on patches of an image, the network is trained on a large dataset of labeled images, i.e. the patches with annotations of specific types of cells. During training, the network is updated to optimize for the cell detection and classification task. The advantage of using YOLOv4 for cell detection and classification on patches of an image is that it can detect multiple cells in a single image patch, and classify them accurately with a high degree of confidence. Additionally, YOLOv4 has a high localization accuracy, which means that it can accurately localize the position of the detected cells in the image patch. Another advantage of YOLOv4 is its ability to detect small objects in an image. This is particularly useful in cell detection, as cells can often be small and difficult to detect. YOLOv4 can handle small objects by using a feature pyramid network, which extracts multi-scale features from the image to detect small objects. Applying the YOLO model to localize objects in selected region of interest (ROI) patches: ResultsThe development of this automated bone marrow cytology system has significant potential to support more efficient and accurate diagnoses in hematology. The results of the study demonstrated that the system achieved high accuracy in region detection, cell detection, and cell classification. The system achieved an accuracy of 0.97 and a ROC AUC of 0.99 in region detection. The mean average precision and average F1-score were 0.75 and 0.78, respectively, with a log-average miss rate of 0.31 in cell detection and classification. The technology can assist pathologists in achieving more accurate diagnoses and treatment plans, ultimately benefiting patients. The automated system has the potential to reduce the time and cost associated with manual bone marrow cytology, making the diagnosis more accessible to patients in rural and remote areas. Generating the Histogram of Cell Types (HCT) and converged Integrated Histogram of Cell Types:","categories":[{"name":"Work","slug":"Work","permalink":"https://kiyoshimu.github.io/categories/Work/"}],"tags":[{"name":"ml","slug":"ml","permalink":"https://kiyoshimu.github.io/tags/ml/"},{"name":"cv","slug":"cv","permalink":"https://kiyoshimu.github.io/tags/cv/"}],"author":"Kiyoshi"},{"title":"Unlocking the Potential of Deep Learning in Pathology Synopses","slug":"pathology_synopses","date":"2021-07-25T04:00:00.000Z","updated":"2023-03-18T17:21:37.057Z","comments":false,"path":"2021/07/25/pathology_synopses/","link":"","permalink":"https://kiyoshimu.github.io/2021/07/25/pathology_synopses/","excerpt":"","text":"Original paper: A BERT model generates diagnostically relevant semantic embeddings from pathology synopses with active learning IntroductionPathology synopses are a crucial aspect of the diagnostic process. They involve experts observing human tissue and summarizing their findings in semi-structured or unstructured text. These synopses are critical in helping physicians make accurate diagnoses and develop appropriate treatment plans. However, the limited number of specialists available to interpret pathology synopses can be a significant barrier to accessing this essential information. Fortunately, deep learning offers a solution to this problem. Main Problems How to build the pathology synopses dataset, as only limited number of experts can interpret these synopses? How to extract info from pathology synopses, which are complex, and require a high degree of domain-specific knowledge to interpret. Solutions AppliedThe overall modeling process: Active learning for pathology synopses datasetWe used an active learning approach was used to develop a set of semantic labels for bone marrow aspirate pathology synopses. In active learning, the machine learning model is not just trained on a fixed dataset, but it is also allowed to query a human expert for labels for selected examples. Here, we start with a small set of labeled examples. These labeled examples would be used to train a machine learning model to map the pathology synopses to semantic labels. The model would then be used to generate predictions for a larger set of unlabeled examples. Rather than simply accepting the model’s predictions, the active learning approach selected a subset of the unlabeled examples that the model is least certain about and present them to a human expert for labeling. By doing this, the machine learning model can learn from the expert’s input and improve its accuracy over time. This process is repeated iteratively, with the machine learning model being updated with the newly labeled examples, and the model making new predictions on the remaining unlabeled examples. This approach significantly reduced the number of labeled examples required to develop an accurate set of semantic labels for pathology synopses, which is essential as it can be time-consuming and expensive to obtain large amounts of labeled data. The active learning process and its result: Transformer for pathology synopsesA transformer-based deep-learning model (BERT) was then trained to map these synopses to one or more semantic labels and to extract learned embeddings from the model’s hidden layer. BERT (Bidirectional Encoder Representations from Transformers) is a state-of-the-art natural language processing (NLP) model that can generate meaningful embeddings from unstructured texts. BERT is based on a self-attention mechanism that allows it to process input sequences in a way that captures the relationships between all of the input tokens. When BERT processes a pathology synopsis, it generates a sequence of hidden states that capture the meaning of each token in the input. These hidden states are then used to generate a fixed-length vector, called an embedding, that represents the meaning of the entire input sequence. The key advantage of BERT over traditional NLP models is that it is pre-trained on large amounts of text data using a masked language modeling task. This pre-training allows BERT to learn general language representations that can be fine-tuned for a specific task, such as mapping pathology synopses to semantic labels. To generate meaningful embeddings from unstructured pathology synopses, BERT is fine-tuned on the labeled dataset of pathology synopses created by active learning. During fine-tuning, the model learns to map the unstructured text input to the correct semantic labels by adjusting the weights of its neural network based on the labeled examples. The learned embeddings generated by BERT capture the semantic meaning of the input sequence in a way that is useful for downstream tasks such as diagnostic classification. By leveraging the power of deep learning and pre-training, BERT can generate embeddings that capture the subtle nuances and context-specific information present in pathology synopses, ultimately leading to more accurate diagnoses and improved patient outcomes. Model performance in embedding extraction: ResultsThe results of the study demonstrated that with a small amount of training data, the transformer-based natural language model was able to extract embeddings from pathology synopses that captured diagnostically relevant information. On average, these embeddings could be used to generate semantic labels mapping patients to probable diagnostic groups with a micro-average F1 score of 0.779 ± 0.025. Pathology synopses play a critical role in the diagnostic process, and their interpretation requires a high degree of domain-specific knowledge. The success of this study provides a generalizable deep learning model and approach that can unlock the semantic information inherent in pathology synopses. By using transformer-based deep-learning models to extract meaningful embeddings from pathology synopses, we can unlock the semantic information inherent in these synopses and improve the accuracy and speed of diagnoses. Model performance in label prediction: Co-occurrence of the predicted labels:","categories":[{"name":"Work","slug":"Work","permalink":"https://kiyoshimu.github.io/categories/Work/"}],"tags":[{"name":"ml","slug":"ml","permalink":"https://kiyoshimu.github.io/tags/ml/"},{"name":"nlp","slug":"nlp","permalink":"https://kiyoshimu.github.io/tags/nlp/"}],"author":"Kiyoshi"},{"title":"Generate Charming Faces From Sketch","slug":"pretty_face","date":"2021-01-02T05:00:00.000Z","updated":"2023-03-18T17:22:29.107Z","comments":false,"path":"2021/01/02/pretty_face/","link":"","permalink":"https://kiyoshimu.github.io/2021/01/02/pretty_face/","excerpt":"","text":"IntroductionA generative adversarial network (GAN) is a class of machine learning frameworks that generate new fake data deceptively from real data. It was invented in just 2014, but its applications have increased rapidly. It has also been used successfully in lots of areas, including fashion, art, advertising, and science. GANs can be used to achieve image-to-image translation, where the generation of the output image is conditional on an input. I think it would be interesting to use segmentation layouts or sketches as inputs to let the model generate pretty faces. Right, pretty or not is a matter of taste). Fortunately, I found one paper, Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation(Encoding in Style), which published recently has paved the road to this imaginary transformation. Main Problems How to build a pretty-face dataset? How to generate face? How to translate sketch to face? Solutions I appliedUse a GAN to create a dataset for another GANThe benefit of GANs is that it can provide unlimited data. If I want to create a dataset with pretty-face, why not use a GAN pre-trained on celebrities. Why celebrities? Because it’s heuristic that there are more good-looking people in the group of stars than in the group of ordinary people. Since a GAN learn the distribution of its training data, it masks sense to select stars as training data. Luckily, when I was searching for a proper dataset, I found someone has already trained a StyleGAN2 on an ideal dataset that has 95600 512*512 faces from 500 Chinese stars. Since not all the stars in the training dataset have beautiful faces, the GAN images are not all pretty. A model’s problem can be solved using another model. Here, I used the model to generate about 3000 pictures and manually selected those that look not bad. Using not bad images and the same number of other images, I prepared a dataset to train a binary classifier using MixNet-S as the backbone. This model can filter qualified faces from the GANs’ products. Next, StyleGAN2 generated another 30,000 images, and the classifier selected 3318 not-bad from them. Finally, these images are collected as the dataset for the following training. Generate Sketch DataMany studies mentioned how to generate sketch data from original data, especially in the community of GANs for line art coloration. Common methods include using sketchKeras, Sketch Simplification etc. Here, I applied the sketchGAN from Sketch Simplification as the authors of Encoding in Style did to make sketches from the selected faces. TrainingThe authors of Encoding in Style did a fantastic job of sharing their training procedure. Just follow their simple instructions, and it’s done. Change the settings in pixel2style2pixel&#x2F;configs&#x2F;data_configs.py Use the command lines Plus: Segmentation LayoutI also tried to use segmentation layouts as inputs to generate images. Here another model is created to create images from segmentation maps (also just following their instruction). The faces’ segmentation layouts are created by using face-parsing Change the settings in pixel2style2pixel&#x2F;configs&#x2F;data_configs.py Use the command lines ResultsPretty face dataset Faces sketches segmentation The dataset has been published on Kaggle. sketch to faces segmentation to faces","categories":[{"name":"Whim","slug":"Whim","permalink":"https://kiyoshimu.github.io/categories/Whim/"}],"tags":[{"name":"ml","slug":"ml","permalink":"https://kiyoshimu.github.io/tags/ml/"},{"name":"cv","slug":"cv","permalink":"https://kiyoshimu.github.io/tags/cv/"}],"author":"Kiyoshi"},{"title":"Crystal Eyes, an Anime Character Recognition System","slug":"crystal_eye","date":"2020-12-22T05:00:00.000Z","updated":"2023-03-18T17:20:54.411Z","comments":false,"path":"2020/12/22/crystal_eye/","link":"","permalink":"https://kiyoshimu.github.io/2020/12/22/crystal_eye/","excerpt":"","text":"IntroductionIt would be cool to find a 2D character’s source anime by using her&#x2F;his images. Some platforms may use this system to help users find animes. Others may use it to check whether one’s newly created character already has his&#x2F;her debut, i.e., the new character is not original enough. One solution is to rely on Google Image’s Image Search, also called reverse image search, which can find similar images and their sources indexed by Google. However, it fails to provide the right source when the input image is an original one. It’s due to the nature of its encoding and indexing method. First, when Google makes embeddings (vectors) for images to find their similar counterparts, I think they will use a general algorithm that fits typical pictures’ traits. However, anime pictures and sketches don’t belong to this group. Thus, this kind of image embeddings doesn’t have the necessary information to represent their semantics. Second, thanks to the emphasis on speed, Google will not use a machine learning model to predict one image’s identity from the embedding. Instead, to the best, they will use distance rules to find other images whose embeddings are close to the input’s embedding. A typical software using distance is Facebook’s faiss. This system has excellent speed performance. However, because it relies on distance computations, such as L2 distance or cos distance, the manipulation it can conduct on the embeddings is limited, which results in a sub-optimal search result. Or, if they think the distance system will still hurt the performance, they would simply use input metadata to match other images. To better meet the demand, we need to make a system that specialized to encode anime characters’ head images and uses machines learning to predict the classes. We use heads instead of the full images because, in this way, we can improve its generalization and performance. The background and cloth information is indeed essential to identify a character. However, in our application, the input may only have head information. It’s critical to let the model’s training data match the future inputs as close as possible to reduce the training&#x2F;serving skew.Also, the computation for the embedding is lighter when the input is the head part than it’s the full picture, which can boost the system’s speed performance. There are a few methods close to this expectation, like Moeflow, but it only supports 100 characters with an accuracy of 70% (unknown whether it is top-N or first-1-precision). Main Problems How to detect characters’ heads from images How to extract semantic embeddings from heads How to make probability predictions from an embedding Solutions I appliedYOLO, for head detectionThere are trained models for face detection, like AniSeg or animeface-2009 and AnimeHeadDetector. The reasons why we didn’t use them are: They are a little bit out of date, considering the state-of-art; More importantly, they detect faces instead of head, except AnimeHeadDetector. Since this’s a project for fun, why not try something new. YOLO is an excellent system for object detection. Officially, the latest version publishes is [V4]. But, I have to admit the training method showing in their Github is too chaotic. Because this’s a project for fun (again), why not use another user-friendly framework yolov5. It’s not an authentical YOLO version. However, the performance is still decent, and its customed training method is pretty clean. Here, I used the predicted label from the previous model and made changes so that the label box cover the whole head. To avoid the model only learn oval&#x2F;ellipse contours to detect head, we added 1-3 randomly colored circles to training images. def add_sphere(im: Image, xmin, ymin, xmax, ymax): draw_time = random.randint(1, 3) for _ in range(draw_time): draw_random_sphere(im, xmin, ymin, xmax, ymax) Metric learning, for embedding extractionTo enable the search of images based on conceptually similar features from the query, we need a similarity function that can measure how similar these different representations are, i.e., the distances among them. Studies have shown that metric learning, which automatically constructs task-specific distance metrics from supervised or weakly supervised data in a machine learning manner, performs well in solving this problem, such as face recognition. It uses ranking losses, such as pairwise ranking lossand triplet ranking loss, to “learn” how to produce similar representations for similar inputs and distant representations for the different inputs. Here, we used the incredible library PyTorch Metric Learning to conduct the training process. From the paper, we learned MultiSimilarityLoss is one of the best loss functions for this task, and its parameters are simple, so we chose it as the loss function. We also followed the recommendation in that paper to use Mean Average Precision at R (MAP@R), which combines the ideas of Mean Average Precision and R-precision as the primary metrics. We used the data from Danbooru 2018 Anime Character Recognition Dataset. We randomly selected 45 images from the characters with more than 50 images, which results in 1551 characters and 67,545 images. The backbone MixNet-S is created by using the library, EfficientNets for PyTorch , which provides pre-trained models and corresponding export methods. Since two hyperparameters can be tuned, we used wandb sweep to assist the process. With its built-in support for Parameter importance and Bayesian optimization, it’s comfortable to compare experiments and tune the hyperparameters without writing custom code. Perceptron, for classificationA single-layer neural network represents the most simple neural network form, and it has a fancy name, Perceptron. Here, since the embeddings are optimized to distinguish one character’s head from that of another, a perceptron should be strong enough to handle the classification. We also made a classifier based on faiss and use the neighborhood to make predictions. We compared the top-5 precision of the two models and found Perceptron performs better. We also tested some traditional machine learning models, such as LinearSVC(slow training), KNeighborsClassifier(model size too large), [RandomForestClassifier](too slow training) and NearestCentroid(inferior performance). We also a pipeline form *making embeddings” to *compare Perceptron and faiss* using MLflow. To be honest, it’s not mature enough yet. And it’s complicated to build a pipeline using MLflow. A glance at the pipeline would be like the following. _get_or_run( &quot;train&quot;, &#123; &quot;train_dir&quot;: train_dir, &quot;val_dir&quot;: val_dir, &quot;epochs&quot;: epochs, &quot;drop_rate&quot;: drop_rate, &quot;drop_connect_rate&quot;: drop_connect_rate, &#125;, git_commit, use_cache=False, ) ... _get_or_run( &quot;export&quot;, &#123;&quot;model_path&quot;: model_path, &quot;out_path&quot;: data_dir / &quot;faceEncoder.onnx&quot;&#125;, git_commit, use_cache=False, ) _get_or_run( &quot;embed&quot;, &#123; &quot;model_path&quot;: model_path, &quot;out_path&quot;: data_dir, &quot;dataset_path&quot;: embed_dataset_path, &#125;, git_commit, use_cache=False, ) ResultsDatasets Anime character head labelled dataset for YOLO training 815 anime character head dataset with more than 99 heads per character for embedder training 1551 anime character head dataset with 45 heads per character for each for classifier training Crystal EyesThis system’s advantage is that the recognition doesn’t rely on indexing, making it lightweight and enabling it to recognize unseen images. To our knowledge, Crystal Eyes is the first system that can recognize more than 1500 2D characters with high performance using machine learning. With 25 images’ embeddings as training data and 20 images’ embeddings as test data for each character, the classifier can achieve 0.860 top-3 accuracy and 0.892 top-5 accuracy. It can recognize colored 2D characters and their sketches but with low confidence. We used the samples Google failed at before as examples. Here, we show a possible application for the system, that is, to tell the user whether the character newly designed is original enough. Right! This face with brown hair and brown eyes is too similar to Misaka, the protagonist in A Certain Scientific Railgun (actually, it’s Misaka’s sketch colored by Colorizer, a GAN system using Tag2Pix). But if we colored the hair with black hair and red eyes, it may turn out to be an original character (just kidding).","categories":[{"name":"Whim","slug":"Whim","permalink":"https://kiyoshimu.github.io/categories/Whim/"}],"tags":[{"name":"ml","slug":"ml","permalink":"https://kiyoshimu.github.io/tags/ml/"},{"name":"cv","slug":"cv","permalink":"https://kiyoshimu.github.io/tags/cv/"}],"author":"Kiyoshi"},{"title":"Kenny the Kidney: A Mobile Health Application","slug":"kidney_app","date":"2020-04-10T04:00:00.000Z","updated":"2023-03-18T15:07:51.184Z","comments":false,"path":"2020/04/10/kidney_app/","link":"","permalink":"https://kiyoshimu.github.io/2020/04/10/kidney_app/","excerpt":"","text":"IntroductionAn increasing number of mobile health (mHealth) applications are being developed, and there are currently several mHealth apps that address a variety of health topics and diseases. A study published by Biomedical Informatics in 2014 has shown mHealth applications are a valuable tool to contribute to patient empowerment and better self-management of chronic diseases. In one of my graduate courses, my team proposed an application that will serve as a central repository for dialysis patients to track more than just their nutritional intake. This App will be a more comprehensive repository of data and resources for dialysis patients, which will enable them to become more educated and successful in managing their disease over time. I was honored to become the so-called chief full-stack developer for this project. Main Problems How to deliver a full-stack App using only one developer? How to provide Accurate visual display and comprehensible notification concerning eGFR and overall renal function? Solutions I appliedI chose microservices, a variant of the service-oriented architecture (SOA) structural style, to arrange our Application as a collection of loosely coupled services. The services include hosting, sending email for doctor verification, doctor confirmation, and patient condition warning. All components interact via their REST API. Framework: AngularAngular is a platform for building mobile and desktop web applications. It has all the libraries to achieve the functions we need. For example, It has AngularFire, an official Angular library for Firebase, which has off-the-shelf Authentication support. It has Angular Material, which offers straightforward APIs for UI with consistent cross-platform behaviors. It has Angular Animations utility library, which provides reusable and parametrized animations that can be used in a declarative manner. Front-end: Firebase HostingThe front-end of our Web App is backed by Firebase Hosting, a production-grade web content hosting. The front-end doesn’t keep state, and all the data are saved in our database in real-time. Using the Firebase CLI, I deployed static content complied by Angular, from local directories on our computers to the hosting server. All content is served over an SSL connection from the closest edge server on Google’s global CDN. Database: FirestoreI leveraged the real-time listeners and offline support provided by Firestore to keep the data in-sync across client Apps. Also, in this way, our App can work regardless of network latency or Internet connectivity. Moreover, after writing data into this database, it can trigger other servers, like “doctor verification”. This database is document-oriented. I designed specific schemas for “doctor”, “eGFR”, “user” and “weight” datasets to fit this feature. For example, the schema of eGFR data implemented by Typescript is as follows. Other schemas can be found at Models in our Git Repo. Results Here, we show images on mobile screens. The web pages will render differently on a variety of devices and window or screen sizes. Because the images on large screens are, of course, large, we only show the home page on large screens as the following. Our App is responsive, i.e., we made web pages render well on a variety of devices and window or screen sizes. For example, when users view the home page on a large screen, the left-side navigation bar and the charts’ legend appear by default, which is hidden on small screens. Service-1: AlertSuppose the patient has a verified doctor and enables the notification function of our App, when the eGFR is in the range of ‘Kidney function moderately to severely decreased’, ‘Kidney function severely decreased’ or ‘Kidney failure’. In that case, an HTTP call will be sent from the user’s device to the “Alert” service, automatically. This service will retrieve the recent data from our database and call a third party API from QuickChart to create a chart for the data. This service will attach the chart and data in an email and send it to the corresponding doctor. Service-2: Doctor ConfirmationSuppose users assign random people as their doctors, without the “Doctor Confirmation” service to let the receivers know they will get Alert emails. In that case, the unlucky people chosen by the bad users may be flooded by alert emails sent from our Alert service. Doctor Confirmation will prevent users from abusing our App in this way. When users add their doctors and enable the notification function, the Doctor Confirmation service will send an email to the possible doctor to ask their confirmation. This email includes a link that can send a “POST” call to the Doctor Verification service. Only after the doctor confirm they are willing to get alerting emails for their patients, the alerting emails will be sent to them. Service-3: Doctor VerificationThis service is called from confirmation emails only. When it’s triggered, it will write the “doctorVerified” in patient’s profile as “Ture”, so that when the patient’s eGFR is in dangerous range, an alter email can be sent to the doctor.","categories":[{"name":"Personal Project","slug":"Personal-Project","permalink":"https://kiyoshimu.github.io/categories/Personal-Project/"}],"tags":[{"name":"firebase","slug":"firebase","permalink":"https://kiyoshimu.github.io/tags/firebase/"},{"name":"angular","slug":"angular","permalink":"https://kiyoshimu.github.io/tags/angular/"}],"author":"Kiyoshi"},{"title":"Avoid Late Flight: A Toy Project","slug":"flight","date":"2019-12-20T05:00:00.000Z","updated":"2023-03-18T15:07:51.184Z","comments":false,"path":"2019/12/20/flight/","link":"","permalink":"https://kiyoshimu.github.io/2019/12/20/flight/","excerpt":"","text":"IntroductionImagine you’re going to a remote place where you will have a super important meeting. If you’ll be late, it’s better to cancel the meeting early (when the airplane’s wheels get off) than letting the CEO or CTO wait for your arrival, annoying that you dare to disrespect them. Still, it’s acceptable for someone to be late for less than 10 minutes. After all, it’s understandable that as a non-machine, it’s hard to be punctual every time. As a result, you wonder if you can predict whether you will be too late. Then, you can have as many not-too-late conferences as possible, that would be so great! This project’s goal is to make this vision become a reality. I built a deep-learning model that can give you a suggestion about whether to cancel a meeting. I know you may want to try it first. Here you go. (If you didn’t receive an email, which means the server on GCP is down to save my money, sorry.) Main Problems How to build an infrastructure that can automatically download and clean data monthly? How to build a deep-learning model in a cloud environment? How to deploy the model as an application? Solutions I applied(Go to https://github.com/YewtsingMu/airline_demo for a more detailed illustration) I used the flight data in 2018 from Transtats. I leveraged the Google Cloud Platform to complete the tasks below: Build microservices that can automatically download and clean data monthly Batch download data of 2018 and processing them Randomly sample 80% data to build a deep-learning model Use half of the rest data to evaluate the model Deploy the model Microservices achieve automatically downloadingIn this part, I prepared the data to train the goal model. Below is the diagram of this part. I used Cloud Schedule, Cloud Pubsub, Cloud Functions, and Cloud Dataflow as microservices together to perform monthly auto-downloading, processing, and saving tasks. The Cloud Scheduler will initiate a message sending to Cloud Pubsub’s “monthly-reminder” topic. When the message comes, it’ll trigger a Cloud Function, downloading the latest monthly data from the Transtats, unzip it, changing the name, and save it as a CSV file to the Cloud Storage. When the new CSV file is successfully saved in Cloud Storage, it’ll trigger another Cloud Function, which will submit a job to Cloud Dataflow. Cloud Dataflow will then digest the new data, do some transformation, and finally, sink it into Bigquery, the data warehouse. Cloud Dataflow – Data Preparision Bigquery + Datastudio – Data ExplorationBigquery is the data warehouse, and its cost is equivalent to the cost of Google Storage. As a result, the cost will not increase even if I store the data in this place to interact with the data. What do I mean by “interact with the data”? In Bigquery, I can query questions I am interested in, like whether some carriers are more likely to have a late arrival or whether seasons can influence the flights’ delay. Besides, the data can be loaded into Data Studio, a visualization tool based on Bigquery. Here, I show the result of my question below. The type of carrier, the season, the distance, and the locations have relations with the delay, which fit my intuition. (I know you will be more excited about something anti-intuition). Moreover, it’s not a static visualization tool. When I say it’s based on Bigquery, I mean you can “query” data by touching these icons. For example, you can select a specific date range. All the visualization will change automatically as a result. From the exploration in Data Studio, I notice that the carriers, locations, department delay, distance, and DateTime more or less can influence the delay of the flight. Therefore, I selected related features, including ‘arr_lat’, ‘dep_lat’, ‘dep_lng’, ‘arr_lng’, ‘DEP_DELAY’, ‘DISTANCE’, ‘hour’, ‘month’, and ‘MKT_UNIQUE_CARRIER’ to train a sophisticated model. But before that, let’s come back to Bigquery. Bigquery is not just a tool providing essential SQL functions. I can build a logistic model here as a baseline for my future sophisticated deep learning model. I used the “rand()” function to sample the training dataset, evaluation dataset, and test dataset. Oh, don’t forget as common sense in machine learning – it’s essential to have balanced negative and positive samples in the training data, i.e., having a similar amount of “cancel” data and “non-cancel” data. Then, I can use the ML training function in Bigquery to train the first basic model for this dataset and see its performance. Don’t be fool by the training result, and the below is the real evaluation performance. Nice, the logistic model can achieve AUC around 0.798. And it’s the baseline I need to break. Model TrainingIn this part, I trained four types of models and deployed the best. Below is the diagram of this part. To train a machine learning model, I think, is just like cooking. If there is a recipe, I should try the existing recipe first. To reinvent pizza is not fun, and the new pizza may not as tasty as the “experienced” pizza. In the AI Platform, there are three build-in algorithms, and I should try them first. The three algorithms are XGBoost, wide and deep model, and linear learner model. XGBoost tree is a kind of traditional machine learning model. Like random forest tree, it’s based on decision tree ensembles, which combine the results of multiple classifications and regression models. The wide and deep model combines a linear model that learns and “memorizes” a wide range of rules with a deep neural network that “generalizes” the rules. A linear learner model assigns one weight to each input feature and sums the weights to predict a numerical target value. See more details. Their final evaluation results are compared as follows. Strangely, this final evaluation in logging for build-in algorithms seems only to use part of the evaluation data. As a result, I have to evaluate the best built-in models by using all the evaluation data. Type Final AUC Evaluation XGBoost 0.865 Linear learner model 0.923 Wide and deep model 0.961 The wide and deep model performs the best here. Now, it’s time to write my code and see whether I could beat Google’s automation. Luckily, mine is better than Google’s automatic data processing and model creation. Here, we can do model examinations to know the model’s pros and cons and then can “targeted” refined the model. For example, use the What-if tool. However, from my experiences, this tool is not friendly with the AI Platform. Also, it’s not directly related to GCP. So, I skip this part. Then, it’s time to deploy the model. Model DeploymentIn this part, I used the Google Sheet as the platform and AI-platform as the backend to deploy my model. Below is the diagram of this part. And the idea of using Google Sheet in this way is from “How to Grow a Spreadsheet into an Application, Google Next 2019“ As mentioned in one of my previous posts, here come at least three solutions. First, simply click the “Deploy Model”. Then GCP will deploy it on ML engines and handle all the other processes. Second, you can deploy the model by yourself, and using docker to make the model service containerized is the option. It’s super fast to write a Dockerfile based on TensorFlow Serving and use Google Build to wrap the model into a container and deploy it to Kubernetes. Last, to achieve full DIY, you can choose to use a VM and do what you want. In this case, you have to set the networking forwarding rules, safety rules, etc.. For simplicity, I selected the first option. Here, it’s worth to notice that to invoke a model deployed on Google ML engines, a role for “ML Engine Developer” is needed. A helpful practice is to create a server account with this role and grant this server account to a microserver on Google Functions. I chose Google Sheet as the platform to create a demo APP. Google Sheet has the database, UI, security, and loads of built-in APIs. To match this platform, I used Google Forms as the APP UI. Wrap UpLet’s wrap up with the whole structure. Better Solutions I know nowConvert the model as Tensorflow.JS and deploy it on the client-side to achieve zero maintenance cost. Use Flutter or Angular to write a more beautiful and customized UI.","categories":[{"name":"Personal Project","slug":"Personal-Project","permalink":"https://kiyoshimu.github.io/categories/Personal-Project/"}],"tags":[{"name":"ml","slug":"ml","permalink":"https://kiyoshimu.github.io/tags/ml/"},{"name":"gcp","slug":"gcp","permalink":"https://kiyoshimu.github.io/tags/gcp/"}],"author":"Kiyoshi"},{"title":"Survival Analysis by Breast Cancer Slides","slug":"survival_analysis","date":"2019-06-12T04:00:00.000Z","updated":"2023-03-18T15:07:51.184Z","comments":false,"path":"2019/06/12/survival_analysis/","link":"","permalink":"https://kiyoshimu.github.io/2019/06/12/survival_analysis/","excerpt":"","text":"IntroductionIn medical research, histology provides essential information for tumor diagnosis and treatments. Anatomic pathologists can classify disease progression by evaluating histological characteristics, such as nuclear atypia, mitotic activity, cell density, and tissue structure. With the advancement of gene sequencing, it is now possible to directly observe the genome’s biomarkers, such as gene expression and epigenetic modification changes, to analyze tumor symptoms. However, histological analysis is still an essential and indispensable tool for predicting the future course of patients. Histological characteristics show the overall impact of molecular-level changes. Meanwhile, the tissue can provide intuitive visual information to help medical staff judge cancer’s invasiveness. However, histological analysis has significant flaws — that is, it is highly subjective and hard to repeat to obtain the same results. In contrast, for computers, the results are derived from a specific series of computations. When the calculation process is unchanged, the same data input will produce the same result. Also, computers can extract all the histological imaging information, while people can only observe specific areas empirically. In this way, computers can use the neglected data in histological imaging to make the diagnosis more comprehensive. Therefore, the analysis of histological imaging by computer can overcome the shortcomings of manual tissue analysis and extract information ignored by experts. As a result, the application of computer analysis of histological imaging to assist medical diagnosis is very promising. However, in most recent studies, experts must label the useful areas of pathological images (Region of Interest). Also, the models they presented are large and complex. Consequently, the models are not easy to use on an ordinary personal computer, limiting their usefulness. Main Problems How to build a model that can provide “time-event” predictions by analyzing breast cancer histological imaging? How to set up a fully automatic “end-to-end” batch processing system, which takes .svs pathological images as input, and survival models as output? How to make this model be trained and used on ordinary computers? Solutions I applied In May 2017, Google Brain designed the AutoML, which can generate artificial intelligence (AI) models. NASNet is a neural network model that Google researchers use reinforcement learning and AutoML as a controller automatically trained. In 2018, NASNet was already the best model in the field of image recognition. So I choose NASNet as the base model to built my Breast Cancer Survival Neural Network Model (SNAS) like below. The raw model. Put genome data into the model, and form the one below. NASNet model has two types, NASNet large model and NASNet mobile model. NASNet mobile is one of the smaller models, and its hardware requirements are smaller, but it can still achieve powerful image recognition functions. To make this model be trained and used on ordinary computers, NASNet mobile model was selected. A system including three parts: segmentation, extraction, and training, is designed to achieve “end-to-end” batch processing. Every piece does not require human intervention. Results The image above shows the classification result of the NSANet classifier. The first line shows the areas classified as useful, and the areas classified as non-useful are in the second line. As you can see, the distinction is evident. Compared to the survival analysis model trained on experts’ extraction features, this model can achieve similar performance. The models with the red marker are which with better performance than the base Cox PH model. There are 78 better than the Cox HP basic model obtained under the same conditions. The records of the performance of 600 models in the figure above show that the performance changes are random. Because each training randomly selects a region from the samples, sometimes the area contains valid information, and sometimes the region does not have valid information. Therefore, although each model’s parameter training is based on the previous model, the latter The trained model is not necessarily better than the previous one. In general, the probability that a model is better than the Cox HP basic model is 13%. If we want 1−(1−0.13)^n ≤ 0.999, we can get n ≥ 50, which means training for 50 times or more, the probability of getting a model with a predictive power better than the Cox HP basic model is higher than 99.9 %. For one case, the randomly selected useful areas are marked in the image below. These green boxes mark them. If we pore over these areas, we get the images below where the individual areas under 10X power imaging are gathered together. The model can be fully automately trained from the .svs pathological slice images. The model is also medium in size, with 7.24e+07 parameters, of which 4.26e+07 parameters are frozen from the model pre-trained on Kaggle data. Hence, the actual number of parameters to be calculated is only 2.97e+07.","categories":[{"name":"Work","slug":"Work","permalink":"https://kiyoshimu.github.io/categories/Work/"}],"tags":[{"name":"ml","slug":"ml","permalink":"https://kiyoshimu.github.io/tags/ml/"}],"author":"Kiyoshi"},{"title":"Edema Feature Extraction From MRI","slug":"edema","date":"2019-03-22T04:00:00.000Z","updated":"2023-03-18T15:39:47.604Z","comments":false,"path":"2019/03/22/edema/","link":"","permalink":"https://kiyoshimu.github.io/2019/03/22/edema/","excerpt":"","text":"IntroductionMagnetic resonance imaging (MRI) is a medical imaging technique used to form pictures of the body’s anatomy. It can be applied to scan the brain to reveal the changes inside. For edema, it is recognized in MRI (T2 weighted or FLAIR pulse series) as a bright signal. Using the computer vision (CV) technique, we can extract and quantify the bright signals. Since these signals reflect each patients’ edema’s traits, doctors may rely on this information to better understand one’s edema’s idiosyncrasy and make suitable decisions. So we, the researchers in Sun Yat-sen Memorial Hospital started this project to study whether the outputs from MRI CV analysis can help doctors to decide which therapy fits the patients’ best interests. We collected coronal 132 series of T2-flair Digital Imaging and Communications in Medicine (DICOM) images from 66 patients before and after their first therapy. My task is to write a program that can extract the bright areas from these images and quantify the features contained within them. Main Problems How to segment the bright areas from DICOM images? How to reconstruct 3D edema models to show the result of segmentation? How to extract features from these regions of interest? Solutions I applied Resize images to 320 * 320, record changes in Spacing Segment edema areas via OpenCV. Calculate volume changes, quantify features via PyRadiomics. Select features related to “Shape”, “ASM”, “Contrast”,“Correlation”, “Homogeneity”, “Entropy”, “Variance”,“Skewness”, “Kurtosis” by T-test. See more details in codes. ResultsThe result of segmentation The result of Reconstruct","categories":[{"name":"Work","slug":"Work","permalink":"https://kiyoshimu.github.io/categories/Work/"}],"tags":[{"name":"ml","slug":"ml","permalink":"https://kiyoshimu.github.io/tags/ml/"},{"name":"cv","slug":"cv","permalink":"https://kiyoshimu.github.io/tags/cv/"}],"author":"Kiyoshi"},{"title":"Eosinophils Detection","slug":"eos_count","date":"2019-01-18T05:00:00.000Z","updated":"2023-03-18T15:07:51.184Z","comments":false,"path":"2019/01/18/eos_count/","link":"","permalink":"https://kiyoshimu.github.io/2019/01/18/eos_count/","excerpt":"","text":"IntroductionThis project was set up by Data and Computer Science department, Sun Yat-sen University, and Sun Yat-sen Memorial Hospital. It aims to build a framework that could train a model to “learn” to detect any types of cells they want. And the first type they chose is the Eosinophils(EOS) , which is vital in clinical testing and somewhat easy to be recognized. Two other medical students and I were invited to try the water. The model was supposed to be used in personal computers, especially laptops. We started with three labeled slides. On each of the slides, there are about 100 - 200 Eosinophils. The figure below is an example. Main Problems How to train a model that can detect Eosinophils in a “reasonable correctness“ (academic study sometimes starts with an ambiguous aim)? How to make this model run on laptops? How to used limited resources for labeling to generate the most labels? Solutions I applied First, I tried to use Google’s object detection API in Tensorflow to train a model, which is based on faster-RCNN architecture. It failed, of course, simply because the number of images is too small. Also, I realized the model of faster-RCNN being one hundred more MB, which is quite large, and it is not possible to run on ordinary laptops fast. Luckily, Kaggle has hosted a competition about cell detections at that moment. From that competition, I met a model architecture, U-net. Many teams in that competition successfully used this architecture. Moreover, it’s fast and relatively small, which fitted our users’ needs. Still, three slides were not enough. So the first thing was to get enough data. We decided to train a simple CNN model to help us label slides. First, we used watershed and other algorithms to segment cell-like “tiles” from the slides. Then, we used these small images to train a classification CNN model. The model worked well. And we used it to classify the cell-like objects segmented by a watershed algorithm on unlabeled slides. It’s an indirect way to label slides. After that, we just corrected the wrong label, and we could get labeled slides super quickly because the CNN model has labeled many cells. The result like the below. The boxed are the model’s labels, and we used green points to “correct” its labels. After 40 to 50 slides, we began to use U-net as the architecture for our next model. However, U-net can only generate a possibility hot-map and needs hot-maps to train. Remember what the users asked is the count of cells, not a possibility hot map. So the U-net model cannot work alone, and we had to design a pipeline to have other components complementing the U-net model. For training, first, we cropped each large slide into small images. Then, we used watershed algorithms to “transform” our previous label as “masks” so that U-net can be trained. When it comes to prediction, we cropped the slide for prediction into small images first; next, we used U-net to generate masks; then, masks were combined into a large mask to match the original slide. Finally, we use watershed and other algorithms to detect the region of cells. Because U-net’s prediction masks will show the interest of regions as white and the background as black, it’s easy for the watershed to separate the cells from this pure background. The final output and its Mask are shown below. Then, we had done it! For more details, look at in Here. ResultsWe randomly sampled 20 well point-labelled images into a test set. By using a different number of training images, we computer how precision, sensitivity and f1_score change as the training image number changes. The “overlap criterion” is defined as an intersection-over-union greater than 0.5. true_positive false_positive false_negative precision sensitivity f1_score training image number 0 1089 837 349 0.565421 0.757302 0.647444 10 1 1116 462 322 0.707224 0.776078 0.740053 15 2 1044 349 394 0.749462 0.726008 0.737549 20 3 1115 600 323 0.650146 0.775382 0.707263 25 4 1079 327 359 0.767425 0.750348 0.75879 30 5 1157 491 281 0.702063 0.80459 0.749838 35 6 1113 357 325 0.757143 0.773992 0.765475 40 7 1072 343 366 0.757597 0.74548 0.75149 45 8 1138 429 300 0.726228 0.791377 0.757404 50 9 969 236 469 0.804149 0.673853 0.733258 55 10 1134 380 304 0.749009 0.788595 0.768293 60 11 1101 326 337 0.771549 0.765647 0.768586 65 12 1012 255 426 0.798737 0.703755 0.748244 70 13 1049 295 389 0.780506 0.729485 0.754134 75 14 1036 242 402 0.810642 0.720445 0.762887 78 Better Solutions I know nowCheck out this article that I published on Medium.","categories":[{"name":"Work","slug":"Work","permalink":"https://kiyoshimu.github.io/categories/Work/"}],"tags":[{"name":"ml","slug":"ml","permalink":"https://kiyoshimu.github.io/tags/ml/"},{"name":"gcp","slug":"gcp","permalink":"https://kiyoshimu.github.io/tags/gcp/"}],"author":"Kiyoshi"}],"categories":[{"name":"Work","slug":"Work","permalink":"https://kiyoshimu.github.io/categories/Work/"},{"name":"Whim","slug":"Whim","permalink":"https://kiyoshimu.github.io/categories/Whim/"},{"name":"Personal Project","slug":"Personal-Project","permalink":"https://kiyoshimu.github.io/categories/Personal-Project/"}],"tags":[{"name":"ml","slug":"ml","permalink":"https://kiyoshimu.github.io/tags/ml/"},{"name":"cv","slug":"cv","permalink":"https://kiyoshimu.github.io/tags/cv/"},{"name":"nlp","slug":"nlp","permalink":"https://kiyoshimu.github.io/tags/nlp/"},{"name":"firebase","slug":"firebase","permalink":"https://kiyoshimu.github.io/tags/firebase/"},{"name":"angular","slug":"angular","permalink":"https://kiyoshimu.github.io/tags/angular/"},{"name":"gcp","slug":"gcp","permalink":"https://kiyoshimu.github.io/tags/gcp/"}]}